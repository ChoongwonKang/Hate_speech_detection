{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT[CLS]+CNN_classification (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, kernel_size=3, dropout_rate=0.2):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=kernel_size)\n",
    "        self.bn = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=kernel_size)\n",
    "        self.fc_input_size = self._calculate_fc_input_size(input_size, kernel_size)\n",
    "        self.hidden1 = nn.Linear(self.fc_input_size, 256)\n",
    "        self.hidden1_bn = nn.BatchNorm1d(256)\n",
    "        self.hidden1_relu = nn.ReLU()\n",
    "        self.hidden1_dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _calculate_fc_input_size(self, input_size, kernel_size):\n",
    "        size = input_size\n",
    "        size = (size - (kernel_size - 1) - 1) + 1\n",
    "        size = size // kernel_size\n",
    "        size = size * 64\n",
    "        return size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)         \n",
    "        x = self.conv(x)           \n",
    "        x = self.bn(x)            \n",
    "        x = self.relu(x)           \n",
    "        x = self.dropout(x)        \n",
    "        x = self.pool(x)           \n",
    "        x = torch.flatten(x, 1)    \n",
    "        x = self.hidden1(x)        \n",
    "        x = self.hidden1_bn(x)     \n",
    "        x = self.hidden1_relu(x)   \n",
    "        x = self.hidden1_dropout(x)\n",
    "        x = self.fc(x)             \n",
    "        return x\n",
    "\n",
    "def load_data(file_path, feature_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    labels = df['encoding'].tolist() \n",
    "    features = np.load(feature_path)\n",
    "    return labels, features\n",
    "\n",
    "def build_dataloader(X, y, batch_size):\n",
    "    tensor_x = torch.tensor(X).float()\n",
    "    tensor_y = torch.tensor(y).long()\n",
    "    dataset = TensorDataset(tensor_x, tensor_y)\n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, device, epochs, early_stopping_patience=3):\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct, total_train = 0, 0, 0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            train_correct += (pred == target).sum().item()\n",
    "            total_train += target.size(0)\n",
    "        \n",
    "        train_accuracy = train_correct / total_train\n",
    "        \n",
    "        val_loss, val_correct, total_val = 0, 0, 0\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item()\n",
    "                _, pred = torch.max(output, dim=1)\n",
    "                val_correct += (pred == target).sum().item()\n",
    "                total_val += target.size(0)\n",
    "   \n",
    "        val_accuracy = val_correct / total_val\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        print(f'Train_loss: {train_loss:.4f} - Train_acc: {train_accuracy:.4f} \\nVal_loss: {val_loss:.4f} - Val_acc: {val_accuracy:.4f}')\n",
    "        print('='*30)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model_cnn.pth\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def process_data_and_train_model(file_path, feature_path, batch_size, epochs, lr, device, seed=42):\n",
    "    set_seed(seed)\n",
    "\n",
    "    labels, features = load_data(file_path, feature_path)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=seed, stratify=labels)\n",
    "    train_loader = build_dataloader(X_train, y_train, batch_size)\n",
    "    val_loader = build_dataloader(X_val, y_val, batch_size)\n",
    "    num_classes = 2 # change to 3 if multi-classification \n",
    "    input_size = 768 # 769 # 849 # input size depends on concatenating liwc or vader vectors\n",
    "    model = CNNClassifier(num_classes=num_classes, input_size=input_size).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay = 1e-2)\n",
    "\n",
    "    train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, device, epochs)\n",
    "\n",
    "# Parameters for model training\n",
    "file_path = \"\" # train_input_file\n",
    "feature_path = \"\" # train_feature_file\n",
    "batch_size = 16 #20\n",
    "epochs = 20\n",
    "lr = 2e-5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "process_data_and_train_model(file_path, feature_path, batch_size, epochs, lr, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, kernel_size=3, dropout_rate=0.2):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=kernel_size)\n",
    "        self.bn = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=kernel_size)\n",
    "        self.fc_input_size = self._calculate_fc_input_size(input_size, kernel_size)\n",
    "        self.hidden1 = nn.Linear(self.fc_input_size, 256)\n",
    "        self.hidden1_bn = nn.BatchNorm1d(256)\n",
    "        self.hidden1_relu = nn.ReLU()\n",
    "        self.hidden1_dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _calculate_fc_input_size(self, input_size, kernel_size):\n",
    "        size = input_size\n",
    "        size = (size - (kernel_size - 1) - 1) + 1\n",
    "        size = size // kernel_size\n",
    "        size = size * 64\n",
    "        return size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)         \n",
    "        x = self.conv(x)          \n",
    "        x = self.bn(x)            \n",
    "        x = self.relu(x)           \n",
    "        x = self.dropout(x)        \n",
    "        x = self.pool(x)           \n",
    "        x = torch.flatten(x, 1)    \n",
    "        x = self.hidden1(x)       \n",
    "        x = self.hidden1_bn(x)     \n",
    "        x = self.hidden1_relu(x)   \n",
    "        x = self.hidden1_dropout(x)\n",
    "        x = self.fc(x)             \n",
    "        return x\n",
    "\n",
    "def load_data(file_path, feature_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    labels = df['encoding'].tolist() \n",
    "    features = np.load(feature_path)\n",
    "    return labels, features\n",
    "\n",
    "def build_dataloader(X, y, batch_size):\n",
    "    tensor_x = torch.tensor(X).float()\n",
    "    tensor_y = torch.tensor(y).long()\n",
    "    dataset = TensorDataset(tensor_x, tensor_y)\n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "def load_best_model(model_path, num_classes, input_size, device):\n",
    "    model = CNNClassifier(num_classes=num_classes, input_size=input_size).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))  \n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_correct, total = 0, 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            total_correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "            all_predictions.extend(pred.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    accuracy = total_correct / total\n",
    "    precision = precision_score(all_targets, all_predictions, average='macro') # change to average='weighted' if multi-classification \n",
    "    recall = recall_score(all_targets, all_predictions, average='macro') # change to average='weighted' if multi-classification \n",
    "    f1 = f1_score(all_targets, all_predictions, average='macro') # change to average='weighted' if multi-classification \n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def test_model(test_file_path, test_feature_path, batch_size, model_path, device):\n",
    "    \n",
    "    test_labels, test_features = load_data(test_file_path, test_feature_path)\n",
    "    test_loader = build_dataloader(test_features, test_labels, batch_size)\n",
    "    num_classes = 2 # change to 3 if multi-classification\n",
    "    input_size = 768 # 769 # 849 # input size depends on concatenating liwc or vader vectors\n",
    "    best_model = load_best_model(model_path, num_classes, input_size, device)\n",
    "    accuracy, precision, recall, f1 = evaluate(best_model, test_loader, device)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "test_file_path = \"\" # test_input_file\n",
    "test_feature_path = \"\" # test_feature_file\n",
    "model_path = \"best_model_cnn.pth\"\n",
    "batch_size = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "test_model(test_file_path, test_feature_path, batch_size, model_path, device)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
